#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para melhorar precisÃ£o do modelo focando em letras similares
Treina modelo com mais dados para letras confusas (A/E, C/O)
"""

import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import pickle
from datetime import datetime
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# ConfiguraÃ§Ãµes do MediaPipe
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

def process_landmarks(hand_landmarks):
    """Process hand landmarks and normalize relative to wrist (landmark 0) - Enhanced version"""
    if not hand_landmarks:
        return None

    # Ponto de referÃªncia (pulso)
    wrist = hand_landmarks.landmark[0]

    # Extrair coordenadas x,y relativas ao pulso (42 features)
    features = []
    for landmark in hand_landmarks.landmark:
        features.extend([
            landmark.x - wrist.x,
            landmark.y - wrist.y
        ])
    
    # Adicionar features extras para melhorar precisÃ£o
    # DistÃ¢ncias entre pontos importantes
    thumb_tip = hand_landmarks.landmark[4]
    index_tip = hand_landmarks.landmark[8]
    middle_tip = hand_landmarks.landmark[12]
    ring_tip = hand_landmarks.landmark[16]
    pinky_tip = hand_landmarks.landmark[20]
    
    # DistÃ¢ncias entre dedos e pulso
    features.extend([
        abs(thumb_tip.x - wrist.x) + abs(thumb_tip.y - wrist.y),
        abs(index_tip.x - wrist.x) + abs(index_tip.y - wrist.y),
        abs(middle_tip.x - wrist.x) + abs(middle_tip.y - wrist.y),
        abs(ring_tip.x - wrist.x) + abs(ring_tip.y - wrist.y),
        abs(pinky_tip.x - wrist.x) + abs(pinky_tip.y - wrist.y)
    ])
    
    # DistÃ¢ncias entre dedos
    features.extend([
        abs(thumb_tip.x - index_tip.x) + abs(thumb_tip.y - index_tip.y),
        abs(index_tip.x - middle_tip.x) + abs(index_tip.y - middle_tip.y),
        abs(middle_tip.x - ring_tip.x) + abs(middle_tip.y - ring_tip.y),
        abs(ring_tip.x - pinky_tip.x) + abs(ring_tip.y - pinky_tip.y)
    ])
    
    return features  # Total: 42 + 5 + 4 = 51 features

def collect_gesture_data_enhanced(gesture_name, samples_per_gesture=50):
    """Coleta dados de um gesto especÃ­fico com instruÃ§Ãµes detalhadas"""
    print(f"ğŸ¯ TREINAMENTO APRIMORADO: {gesture_name}")
    print("=" * 60)
    
    # InstruÃ§Ãµes especÃ­ficas para letras problemÃ¡ticas
    instructions = {
        'A': "GESTO A: Punho fechado com polegar estendido para cima (como 'ok')",
        'E': "GESTO E: Todos os dedos estendidos e juntos (como 'pare')",
        'C': "GESTO C: MÃ£o em formato de C (dedos curvados como uma concha)",
        'O': "GESTO O: MÃ£o em formato de O (dedos curvados formando cÃ­rculo fechado)"
    }
    
    instruction = instructions.get(gesture_name, f"FaÃ§a o gesto de {gesture_name} em LIBRAS")
    
    print("ğŸ“‹ INSTRUÃ‡Ã•ES ESPECÃFICAS:")
    print(f"   {instruction}")
    print("ğŸ“‹ COMANDOS:")
    print("   â€¢ Pressione ESPAÃ‡O para capturar")
    print("   â€¢ Pressione 'q' para sair")
    print("   â€¢ Pressione 'r' para reiniciar contagem")
    print("=" * 60)
    
    # Inicializar MediaPipe
    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=1,
        min_detection_confidence=0.8,  # Maior precisÃ£o
        min_tracking_confidence=0.7
    )
    
    # Inicializar cÃ¢mera
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("âŒ Erro: NÃ£o foi possÃ­vel abrir a cÃ¢mera")
        return []
    
    print("âœ… CÃ¢mera inicializada")
    
    # Lista para armazenar dados
    gesture_data = []
    sample_count = 0
    
    print(f"ğŸ“Š Coletando {samples_per_gesture} amostras do gesto {gesture_name}...")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        # Flip horizontal para espelho
        frame = cv2.flip(frame, 1)
        
        # Converter BGR para RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Processar com MediaPipe
        results = hands.process(rgb_frame)
        
        # Desenhar landmarks se detectados
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
        
        # InformaÃ§Ãµes na tela
        cv2.putText(frame, f"GESTO: {gesture_name} - Amostras: {sample_count}/{samples_per_gesture}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(frame, instruction[:50] + "...", 
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        cv2.putText(frame, "ESPACO=capturar | Q=sair | R=reiniciar", 
                   (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        # Mostrar frame
        cv2.imshow(f'Treinamento Aprimorado - {gesture_name}', frame)
        
        # Capturar teclas
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord(' '):  # EspaÃ§o para capturar
            if results.multi_hand_landmarks:
                landmarks = results.multi_hand_landmarks[0]
                features = process_landmarks(landmarks)
                
                if features and len(features) == 51:
                    gesture_data.append(features)
                    sample_count += 1
                    print(f"âœ… Amostra {sample_count} capturada!")
                    
                    if sample_count >= samples_per_gesture:
                        print(f"ğŸ‰ Coleta de {gesture_name} concluÃ­da!")
                        break
                else:
                    print("âŒ Erro: NÃ£o foi possÃ­vel processar landmarks")
            else:
                print("âŒ Erro: MÃ£o nÃ£o detectada")
        
        elif key == ord('r'):  # Reiniciar contagem
            sample_count = 0
            gesture_data = []
            print("ğŸ”„ Contagem reiniciada!")
        
        elif key == ord('q'):
            break
    
    # Limpar recursos
    cap.release()
    cv2.destroyAllWindows()
    hands.close()
    
    return gesture_data

def create_enhanced_model():
    """Cria modelo aprimorado com foco em letras similares"""
    print("ğŸš€ CRIAÃ‡ÃƒO DO MODELO APRIMORADO")
    print("=" * 60)
    
    # Lista de gestos com foco nas letras problemÃ¡ticas
    gestures = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y']
    
    # Mais amostras para letras problemÃ¡ticas
    samples_per_gesture = {
        'A': 80,  # Mais amostras para A
        'E': 80,  # Mais amostras para E
        'C': 80,  # Mais amostras para C
        'O': 80,  # Mais amostras para O
        'ESPACO': 50
    }
    
    all_data = []
    all_labels = []
    
    for gesture in gestures:
        samples = samples_per_gesture.get(gesture, 40)  # 40 amostras para outras letras
        print(f"\nğŸ“ Treinando gesto: {gesture} ({samples} amostras)")
        gesture_data = collect_gesture_data_enhanced(gesture, samples)
        
        if gesture_data:
            all_data.extend(gesture_data)
            all_labels.extend([gesture] * len(gesture_data))
            print(f"âœ… {gesture}: {len(gesture_data)} amostras coletadas")
        else:
            print(f"âŒ {gesture}: Nenhuma amostra coletada")
    
    if not all_data:
        print("âŒ Nenhum dado coletado!")
        return
    
    print(f"\nğŸ“Š Total de amostras coletadas: {len(all_data)}")
    print(f"ğŸ“Š Classes: {set(all_labels)}")
    
    # Converter para arrays numpy
    X = np.array(all_data)
    y = np.array(all_labels)
    
    # Dividir dados
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"ğŸ“Š Dados de treino: {len(X_train)} amostras")
    print(f"ğŸ“Š Dados de teste: {len(X_test)} amostras")
    
    # Treinar modelo com parÃ¢metros otimizados
    print("\nğŸ¤– Treinando modelo aprimorado...")
    model = RandomForestClassifier(
        n_estimators=200,  # Mais Ã¡rvores
        random_state=42,
        max_depth=15,      # Profundidade maior
        min_samples_split=3,  # Menos amostras para split
        min_samples_leaf=1,   # Menos amostras por folha
        max_features='sqrt',  # OtimizaÃ§Ã£o de features
        class_weight='balanced'  # Balanceamento de classes
    )
    
    model.fit(X_train, y_train)
    
    # Avaliar modelo
    accuracy = model.score(X_test, y_test)
    print(f"ğŸ¯ AcurÃ¡cia geral: {accuracy:.2%}")
    
    # RelatÃ³rio detalhado
    y_pred = model.predict(X_test)
    print("\nğŸ“Š RELATÃ“RIO DETALHADO:")
    print(classification_report(y_test, y_pred))
    
    # Matriz de confusÃ£o para letras problemÃ¡ticas
    print("\nğŸ” MATRIZ DE CONFUSÃƒO (Letras ProblemÃ¡ticas):")
    problem_letters = ['A', 'E', 'C', 'O']
    problem_mask = np.isin(y_test, problem_letters)
    if np.any(problem_mask):
        cm = confusion_matrix(y_test[problem_mask], y_pred[problem_mask], labels=problem_letters)
        print("ConfusÃ£o entre A/E/C/O:")
        for i, letter in enumerate(problem_letters):
            print(f"{letter}: {cm[i]}")
    
    # Obter classes Ãºnicas
    classes = sorted(list(set(y)))
    print(f"\nğŸ“Š Classes finais: {classes}")
    
    # Salvar modelo
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    model_filename = f'modelos/modelo_aprimorado_{timestamp}.pkl'
    info_filename = f'modelos/modelo_info_aprimorado_{timestamp}.pkl'
    
    with open(model_filename, 'wb') as f:
        pickle.dump(model, f)
    
    model_info = {
        'classes': classes,
        'num_classes': len(classes),
        'features': 51,  # 51 features aprimoradas
        'accuracy': accuracy,
        'timestamp': timestamp,
        'description': 'Modelo aprimorado com foco em letras similares',
        'enhanced_features': True
    }
    
    with open(info_filename, 'wb') as f:
        pickle.dump(model_info, f)
    
    print(f"\nâœ… Modelo salvo: {model_filename}")
    print(f"âœ… Info salva: {info_filename}")
    
    # Salvar dados
    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(51)])
    df['label'] = y
    df.to_csv('gestos_aprimorados.csv', index=False)
    
    print(f"âœ… Dados salvos: gestos_aprimorados.csv")
    
    return model_filename, info_filename

if __name__ == "__main__":
    model_file, info_file = create_enhanced_model()
    
    if model_file and info_file:
        print("\nğŸ‰ TREINAMENTO APRIMORADO CONCLUÃDO!")
        print("=" * 60)
        print(f"ğŸ“ Modelo: {model_file}")
        print(f"ğŸ“ Info: {info_file}")
        print("ğŸ”„ Agora vocÃª pode atualizar o app.py para usar o novo modelo")
        print("ğŸ’¡ O modelo tem 51 features e foco especial em letras similares")
    else:
        print("\nâŒ TREINAMENTO FALHOU!")
