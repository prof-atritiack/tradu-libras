<p align="center">
  <img src="https://img.shields.io/badge/python-3.10-blue?logo=python" />
  <img src="https://img.shields.io/badge/mediapipe-informational?logo=google" />
  <img src="https://img.shields.io/badge/opencv-4.x-green?logo=opencv" />
  <img src="https://img.shields.io/badge/scikit--learn-ML-orange?logo=scikit-learn" />
   <img src="https://img.shields.io/badge/status-em%20desenvolvimento-lightgrey" />
</p>

# TraduLibras

Sistema de reconhecimento de LÃ­ngua Brasileira de Sinais (LIBRAS) usando visÃ£o computacional.

## ðŸ“‹ PrÃ©-requisitos

- Python 3.10 ou superior
- Webcam funcionando
- Sistema operacional: Windows, Linux ou macOS
- ConexÃ£o com a internet (para a primeira instalaÃ§Ã£o)

## ðŸš€ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/prof-atritiack/libras-js.git
cd libras-js
```

2. Crie um ambiente virtual Python:
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/macOS
python3 -m venv .venv
source .venv/bin/activate
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

## ðŸŽ¯ Treinamento do Modelo de Reconhecimento

### ðŸ“ Passo 1: Coleta de Dados de Gestos

Para treinar o modelo, vocÃª precisa coletar dados dos gestos em LIBRAS:

1. **Execute o script de coleta:**
```bash
# Ative o ambiente virtual primeiro
.venv\Scripts\activate  # Windows
# ou
source .venv/bin/activate  # Linux/macOS

# Execute o script de treinamento
python treinar_letras_simples.py
```

2. **Processo de coleta:**
   - A cÃ¢mera serÃ¡ ativada automaticamente
   - Para cada letra (A, B, C, L, Y), vocÃª verÃ¡:
     - Nome da letra na tela
     - Contador de amostras coletadas
     - Pontos de referÃªncia da mÃ£o desenhados
   
3. **Como coletar amostras:**
   - Posicione sua mÃ£o no centro da cÃ¢mera
   - FaÃ§a o gesto da letra correspondente
   - Pressione **ESPAÃ‡O** para capturar uma amostra
   - Pressione **ESC** para pular uma letra
   - **Recomendado:** 30-50 amostras por letra

4. **Dicas para melhor coleta:**
   - Varie a posiÃ§Ã£o e Ã¢ngulo da mÃ£o
   - Mantenha boa iluminaÃ§Ã£o
   - Evite movimentos bruscos
   - Certifique-se de que a mÃ£o estÃ¡ bem visÃ­vel

### ðŸ¤– Passo 2: Treinamento AutomÃ¡tico

ApÃ³s coletar os dados, o modelo serÃ¡ treinado automaticamente:

1. **O que acontece automaticamente:**
   - DivisÃ£o dos dados em treino (80%) e teste (20%)
   - Treinamento do modelo Random Forest
   - AvaliaÃ§Ã£o da acurÃ¡cia
   - Salvamento do modelo em `modelos/modelo_libras.pkl`

2. **Resultados esperados:**
   - AcurÃ¡cia no treinamento: ~100%
   - AcurÃ¡cia no teste: ~95-100%
   - Modelo salvo e pronto para uso

### ðŸ”§ Passo 3: VerificaÃ§Ã£o do Modelo

Para verificar se o modelo foi treinado corretamente:

```bash
python -c "
import pickle
import pandas as pd

# Carregar modelo
model = pickle.load(open('modelos/modelo_libras.pkl', 'rb'))
print('âœ… Modelo carregado com sucesso!')
print(f'ðŸ“Š Tipo: {type(model)}')
print(f'ðŸ”¤ Classes: {model.classes_}')

# Verificar dados
df = pd.read_csv('gestos_libras.csv')
print(f'ðŸ“ Amostras: {len(df)}')
print(f'ðŸ·ï¸  Classes: {sorted(df[\"label\"].unique())}')
"
```

## ðŸ’» Executando o projeto

1. **Ative o ambiente virtual:**
```bash
# Windows
.venv\Scripts\activate

# Linux/macOS
source .venv/bin/activate
```

2. **Execute a aplicaÃ§Ã£o:**
```bash
python app.py
```

3. **Acesse no navegador:**
```
http://localhost:5000
```

## âš¡ Comandos RÃ¡pidos

### ðŸš€ Iniciar o projeto:
```bash
# Ativar ambiente virtual
.venv\Scripts\activate  # Windows
source .venv/bin/activate  # Linux/macOS

# Executar aplicaÃ§Ã£o
python app.py
```

### ðŸŽ¯ Treinar modelo:
```bash
# Coletar dados e treinar
python treinar_letras_simples.py

# Verificar modelo
python -c "import pickle; model=pickle.load(open('modelos/modelo_libras.pkl','rb')); print(f'Classes: {model.classes_}')"
```

### ðŸ”§ Comandos Ãºteis:
```bash
# Verificar dependÃªncias
pip list | findstr -i "flask opencv mediapipe scikit"

# Limpar cache Python
python -m pip cache purge

# Reinstalar dependÃªncias
pip uninstall -r requirements.txt -y && pip install -r requirements.txt
```

## ðŸ“± Usando o TraduLibras

1. Na pÃ¡gina inicial, clique em "ComeÃ§ar Agora" ou acesse a seÃ§Ã£o "CÃ¢mera"
2. Permita o acesso Ã  sua webcam quando solicitado
3. Posicione sua mÃ£o no centro da cÃ¢mera
4. FaÃ§a os sinais das letras em LIBRAS
5. O sistema reconhecerÃ¡ as letras e formarÃ¡ palavras
6. Use o botÃ£o "Falar" para ouvir o texto reconhecido
7. Use "Limpar" para recomeÃ§ar

## ðŸ” Funcionalidades

- Reconhecimento em tempo real de letras em LIBRAS
- Interface web moderna e responsiva
- ConversÃ£o de texto para fala
- Tutorial interativo
- Feedback visual em tempo real

## ðŸ› ï¸ Tecnologias Utilizadas

- **Flask** - Framework web para a interface
- **OpenCV** - Processamento de imagem e captura de vÃ­deo
- **MediaPipe** - DetecÃ§Ã£o e rastreamento de mÃ£os
- **Scikit-learn** - Modelo de machine learning (Random Forest)
- **gTTS** - ConversÃ£o de texto para fala em portuguÃªs
- **HTML/CSS/JavaScript** - Interface do usuÃ¡rio responsiva

## ðŸ“ Estrutura do Projeto

```
tradu-libras/
â”œâ”€â”€ app.py                          # AplicaÃ§Ã£o Flask principal
â”œâ”€â”€ treinar_letras_simples.py       # Script de coleta e treinamento
â”œâ”€â”€ gestos_libras.csv               # Dataset de gestos coletados
â”œâ”€â”€ modelo_libras.pkl               # Modelo treinado (raiz)
â”œâ”€â”€ modelos/                        # DiretÃ³rio de modelos
â”‚   â”œâ”€â”€ modelo_libras.pkl           # Modelo treinado (atual)
â”‚   â””â”€â”€ modelo_info.pkl             # InformaÃ§Ãµes do modelo
â”œâ”€â”€ requirements.txt                # DependÃªncias Python
â”œâ”€â”€ templates/                      # Templates HTML
â”‚   â”œâ”€â”€ index.html                  # PÃ¡gina inicial
â”‚   â”œâ”€â”€ camera.html                 # Interface de reconhecimento
â”‚   â”œâ”€â”€ tutorial.html               # Tutorial do sistema
â”‚   â””â”€â”€ configuracoes.html          # ConfiguraÃ§Ãµes
â”œâ”€â”€ static/                         # Arquivos estÃ¡ticos
â”‚   â”œâ”€â”€ css/                        # Estilos CSS
â”‚   â””â”€â”€ images/                     # Imagens e Ã­cones
â””â”€â”€ README.md                       # DocumentaÃ§Ã£o
```

## ðŸ”„ Fluxo de Trabalho

### 1. **Coleta de Dados** (`treinar_letras_simples.py`)
- Captura gestos via webcam
- Normaliza coordenadas relativas ao pulso
- Salva dados em `gestos_libras.csv`

### 2. **Treinamento** (automÃ¡tico)
- Carrega dados do CSV
- Divide em treino/teste (80/20)
- Treina modelo Random Forest
- Salva modelo em `modelos/`

### 3. **Reconhecimento** (`app.py`)
- Carrega modelo treinado
- Processa vÃ­deo em tempo real
- Detecta gestos com MediaPipe
- Classifica com modelo treinado
- Exibe resultados na interface web

## âš ï¸ Requisitos do Sistema

### Requisitos MÃ­nimos:
- Processador: Dual Core 2GHz
- MemÃ³ria RAM: 4GB
- Webcam: 720p
- EspaÃ§o em disco: 500MB

### Requisitos Recomendados:
- Processador: Quad Core 2.5GHz
- MemÃ³ria RAM: 8GB
- Webcam: 1080p
- EspaÃ§o em disco: 1GB

## ðŸ”§ SoluÃ§Ã£o de Problemas

### A webcam nÃ£o inicia:
1. Verifique se sua webcam estÃ¡ conectada
2. Certifique-se de que nenhum outro programa estÃ¡ usando a cÃ¢mera
3. Recarregue a pÃ¡gina
4. Reinicie a aplicaÃ§Ã£o

### Reconhecimento impreciso:
1. Verifique a iluminaÃ§Ã£o do ambiente
2. Mantenha sua mÃ£o a aproximadamente 50cm da cÃ¢mera
3. Evite movimentos bruscos
4. Certifique-se de que nÃ£o hÃ¡ objetos ou pessoas no fundo
5. **Retreine o modelo** com mais amostras se necessÃ¡rio

### Erro ao instalar dependÃªncias:
1. Verifique sua conexÃ£o com a internet
2. Atualize o pip: `python -m pip install --upgrade pip`
3. Tente instalar as dependÃªncias uma a uma

### Erro no modelo:
1. Verifique se o arquivo `modelos/modelo_libras.pkl` existe
2. Execute novamente o treinamento: `python treinar_letras_simples.py`
3. Certifique-se de que coletou dados suficientes (mÃ­nimo 20 amostras por letra)

## ðŸ†• Adicionando Novas Letras

Para adicionar novas letras ao reconhecimento:

1. **Edite o arquivo `treinar_letras_simples.py`:**
```python
# Modifique a lista FRASES para incluir suas novas letras
FRASES = [
    "Oi Conselho Britanico",
    "TraduLibras",
    "SUA NOVA FRASE AQUI"  # Adicione aqui
]
```

2. **Execute o treinamento novamente:**
```bash
python treinar_letras_simples.py
```

3. **Coleta de dados:**
   - O sistema mostrarÃ¡ automaticamente as novas letras
   - Colete 30-50 amostras para cada nova letra
   - Siga as mesmas dicas de coleta

4. **VerificaÃ§Ã£o:**
   - O modelo serÃ¡ retreinado com todas as letras
   - Verifique a acurÃ¡cia no final do treinamento

## ðŸ“Š Melhorando a PrecisÃ£o

### Para melhorar a precisÃ£o do reconhecimento:

1. **Mais dados de treinamento:**
   - Colete 50-100 amostras por letra
   - Varie posiÃ§Ãµes, Ã¢ngulos e iluminaÃ§Ã£o
   - Inclua diferentes pessoas se possÃ­vel

2. **Qualidade dos dados:**
   - Mantenha gestos consistentes
   - Evite movimentos durante a captura
   - Use boa iluminaÃ§Ã£o uniforme

3. **ParÃ¢metros do modelo:**
   - Edite `treinar_letras_simples.py` para ajustar:
     - `n_estimators`: nÃºmero de Ã¡rvores (padrÃ£o: 100)
     - `max_depth`: profundidade mÃ¡xima (padrÃ£o: 10)
     - `min_samples_split`: amostras mÃ­nimas para divisÃ£o (padrÃ£o: 5)

4. **ValidaÃ§Ã£o cruzada:**
   - Execute o treinamento vÃ¡rias vezes
   - Compare as acurÃ¡cias obtidas
   - Use o modelo com melhor performance

## ðŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## âœ¨ Contribuindo

1. FaÃ§a um Fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ðŸ‘¥ Equipe

Desenvolvido com â¤ï¸ pela equipe TraduLibras

## Tradutor de Libras com IA usando MediaPipe + Scikit-learn

Um projeto educacional que integra **visÃ£o computacional**, **machine learning** e **sÃ­ntese de voz** para traduzir gestos da LÃ­ngua Brasileira de Sinais (Libras) em letras, palavras e frases. Ideal para demonstraÃ§Ãµes de acessibilidade, inclusÃ£o digital e ensino tÃ©cnico.

---

## ðŸš€ Funcionalidades jÃ¡ implementadas

âœ… Coleta de gestos com webcam e rotulagem manual  
âœ… DetecÃ§Ã£o de mÃ£os com **MediaPipe**  
âœ… NormalizaÃ§Ã£o dos dados baseada no punho  
âœ… Treinamento de modelo com **Random Forest** (scikit-learn)  
âœ… Reconhecimento em tempo real de letras com estabilizaÃ§Ã£o  
âœ… Estrutura inicial para acÃºmulo de letras â†’ palavras

---

## 2ï¸âƒ£ Como rodar o projeto

### â–¶ï¸ Etapa 1 â€“ Coletar os dados dos gestos

```bash
python coletar_gestos.py
```

- A cÃ¢mera serÃ¡ ativada.
- Posicione a mÃ£o com o gesto correspondente Ã  letra desejada.
- Pressione a tecla da letra (`A`, `B`, etc.).
- O dado serÃ¡ salvo automaticamente em `gestos_libras.csv`.

ðŸ“Œ Recomendado: coletar entre **30 a 50 amostras por letra** com variaÃ§Ã£o de posiÃ§Ã£o e Ã¢ngulo.

---

### ðŸ§  Etapa 2 â€“ Treinar o modelo

```bash
python treinar_modelo.py
```

- Aplica balanceamento entre as letras.
- Treina um modelo com scikit-learn.
- Salva como `modelo_libras.pkl`.

---

### ðŸ”´ Etapa 3 â€“ Reconhecimento em tempo real

```bash
python reconhecer_em_tempo_real.py
```

- Reconhece a mÃ£o com MediaPipe.
- Identifica e exibe a letra.
- Usa verificaÃ§Ã£o de estabilidade para reduzir ruÃ­do.

---

## ðŸ“ Estrutura do projeto

```bash
tradutor-libras/
â”œâ”€â”€ coletar_gestos.py              # Coleta e rotulagem dos gestos
â”œâ”€â”€ treinar_modelo.py              # Treinamento do modelo de IA
â”œâ”€â”€ reconhecer_em_tempo_real.py    # Reconhecimento com webcam
â”œâ”€â”€ modelo_libras.pkl              # Modelo treinado (Random Forest)
â”œâ”€â”€ gestos_libras.csv              # Dataset dos gestos (normalizado)
â”œâ”€â”€ requirements.txt               # [pendente] Lista de dependÃªncias
â”œâ”€â”€ streamlit_app.py               # [pendente] Interface web
â”œâ”€â”€ audio_output.py                # [pendente] GeraÃ§Ã£o de Ã¡udio com gTTS
â”œâ”€â”€ frase_mapping.py               # [pendente] Mapeamento de palavras para frases
â””â”€â”€ README.md                      # DocumentaÃ§Ã£o do projeto
```

---

## ðŸ§± Arquitetura do Projeto

```mermaid
graph LR
    CAM[Webcam + MediaPipe] --> COLETA[Coleta de Dados]
    COLETA --> CSV[Arquivo CSV]
    CSV --> TREINAMENTO[Modelo Random Forest]
    TREINAMENTO --> PKL[modelo_libras.pkl]
    PKL --> RECOGNITION[Reconhecimento em tempo real]
    RECOGNITION --> LETRA[Letra reconhecida]
    LETRA --> PALAVRA[FormaÃ§Ã£o de palavra]
    PALAVRA --> FRASE[Frase mapeada]
    FRASE --> AUDIO[gTTS: voz]


